# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UtkikI18V6LrMjGjFmpQ_1uU8bW6gLJ5
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

train = pd.read_csv('https://raw.githubusercontent.com/sharmaroshan/Twitter-Sentiment-Analysis/master/train_tweet.csv')
test = pd.read_csv('https://raw.githubusercontent.com/sharmaroshan/Twitter-Sentiment-Analysis/master/test_tweets.csv')

print(train.shape)
print(test.shape)

test.head()

train.head()

train.isnull().any()
test.isnull().any()

train[train['label'] == 0].head(10)

train[train['label'] == 1].head(10)

train['label'].value_counts().plot.bar(figsize = (6, 4))

length_train = train['tweet'].str.len().plot.hist(color = "pink", figsize=(6, 4))
length_test = test['tweet'].str.len().plot.hist(color = 'blue', figsize = (6, 4))

train['len'] = train['tweet'].str.len()
test['len'] = test['tweet'].str.len()
train.head()
train.groupby('label').describe()

train.groupby('len').mean()['label'].plot.hist()
plt.title('variation of length')
plt.xlabel('Length')
plt.show()

from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(stop_words = "english")
words = cv.fit_transform(train.tweet)
sum_words = words.sum(axis = 0)

words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]
words_freq = sorted(words_freq, key=lambda x:x[1], reverse = True)

frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])
frequency.head(30).plot(x = 'word', y="freq", kind="bar", figsize=(15, 7))
plt.title("Frequent words")