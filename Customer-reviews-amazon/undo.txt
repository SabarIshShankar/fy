# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zgOEEOCnX0WeF6fq5Ia-wH0Du2q9iLR1
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import math
import warnings

warnings.filterwarnings('ignore')
warnings.filterwarnings("ignore", category = DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)
sns.set_style("whitegrid")
# %matplotlib inline
np.random.seed(7)

df = pd.read_csv('https://raw.githubusercontent.com/Arjun-Mota/amazon-product-reviews-sentiment-analysis/master/1429_1.csv')
df.head(5)

#we can label each review based on sentiment
#positive and negative

data = df.copy()
data.describe()

"""To analayze the review data to make decisions - products context"""

data.info()

data["asins"].unique() #Amazon Standard Identification Numbe

asins_unique = len(data["asins"].unique())
print("Number of unique asins" + str(asins_unique))

data.hist(bins=50, figsize=(20, 15))
plt.show()

from sklearn.model_selection import StratifiedShuffleSplit
#train a sentiment analysis classifier
#make sure we split such that the classifier is not imbalanced
print("Before {}".format(len(data)))
dataAfter = data.dropna(subset=["reviews.rating"])
print("After {}".format(len(dataAfter)))
dataAfter["reviews.rating"] = dataAfter["reviews.rating"].astype(int)

split = StratifiedShuffleSplit(n_splits = 5, test_size = 0.2)
for train_index, test_index in split.split(dataAfter, dataAfter["reviews.rating"]):
  strat_train = dataAfter.reindex(train_index)
  strat_test = dataAfter.reindex(test_index)

len(strat_train)

strat_train["reviews.rating"].value_counts()/len(strat_train)
print(strat_test)
#based on column

strat_test["reviews.rating"].value_counts()/len(strat_test)

reviews = strat_train.copy()
reviews.head()

len(reviews["name"].unique()), len(reviews["asins"].unique())

reviews.groupby("asins")["name"].unique()

different_names = reviews[reviews["asins"] == "B00L9EPT8O,B01E6AO69U"]["name"].unique()
for name in different_names:
    print(name)

fig = plt.figure(figsize=(16, 10))
ax1 = plt.subplot(211)
ax2 = plt.subplot(212, sharex = ax1)
reviews["asins"].value_counts().plot(kind="bar", ax=ax1, title="ASIN Frequency")
np.log10(reviews["asins"].value_counts()).plot(kind="bar", ax=ax2, title="ASIN Frequency(Log10 Adjusted")

reviews["reviews.rating"].mean()

asins_count = reviews["asins"].value_counts().index
plt.subplots(2, 1, figsize=(16, 12))
plt.subplot(2, 1, 1)
reviews["asins"].value_counts().plot(kind="bar", title="ASIN Frequency")
plt.subplot(2, 1, 2)

plt.subplot(2, 1, 2)
sns.pointplot(x = "asins", y="reviews.rating", order=asins_count, data=reviews)
plt.xticks(rotation=90)
plt.show()

plt.subplots(2,1, figsize=(16, 12))
plt.subplot(2, 1, 1)
reviews["asins"].value_counts().plot(kind="bar", title="ASIN Frequency")
plt.subplot(2,1,2)
sns.pointplot(x="asins", y="reviews.doRecommend", order=asins_count, data=reviews)
plt.xticks(rotation=90)
plt.show()

#correlations
corr_matrix = reviews.corr()
corr_matrix

reviews.info()

counts  = reviews['asins'].value_counts().to_frame()
counts.head()

avg_rating = reviews.groupby("asins")["reviews.rating"].mean().to_frame()
avg_rating.head()

table = counts.join(avg_rating)
plt.scatter("asins", "reviews.rating", data=table)
table.corr()

#sentiment analysis
def sentiments(rating):
  if(rating == 5) or (rating==4):
    return "Positive"
  elif rating  == 3:
    return "Neutral"
  elif (rating == 2) or (rating == 1):
    return "Negative"

strat_train["Sentiment"] = strat_train["reviews.rating"].apply(sentiments)
strat_test["Sentiment"] = strat_test["reviews.rating"].apply(sentiments)
strat_train["Sentiment"][:20]

# Prepare data
X_train = strat_train["reviews.text"]
X_train_targetSentiment = strat_train["Sentiment"]
X_test = strat_test["reviews.text"]
X_test_targetSentiment = strat_test["Sentiment"]
print(len(X_train), len(X_test))

#Feature extraction - reducing larger data into smaller components
X_train = X_train.fillna('')
X_test = X_test.fillna('')
X_train_targetSentiment = X_train_targetSentiment.fillna('')
X_test_target_targetSentiment = X_test_targetSentiment.fillna('')

from sklearn.feature_extraction.text import CountVectorizer
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(X_train)
X_train_counts.shape

#27k training samples, and 12k distinct words in the training sample
#tfidf transformer is used to reduce the redundancy
#term frequencies -  term freq. times inverse document frequency
from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer = TfidfTransformer(use_idf = False)
X_train_tfidf =  tfidf_transformer.fit_transform(X_train_counts)
X_train_tfidf.shape

#building a pipeline from the extracted features
#multinomial naive bayes = classifier = where words are represented as word vector counts
#conditions are independent to each other

from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline #assemble steps to be cross validated with several paramenters
clf_multiNB_pipe = Pipeline([("vect", CountVectorizer()), ("tfidf", TfidfTransformer()), ("clf_nominalNB", MultinomialNB())])
clf_multiNB_pipe.fit(X_train, X_train_targetSentiment)

predictedMultiNB = clf_multiNB_pipe.predict(X_test)
np.mean(predictedMultiNB == X_test_targetSentiment)

#testing
#logistic regression classifier- predict the outcome of a variable
#support vetor machine classifier
#random forest classifier

#logistic regression classifier
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
clf_logReg_pipe = Pipeline([("vect", CountVectorizer()), ("tfidf", TfidfTransformer()), ("clf_logRef", LogisticRegression())])
clf_logReg_pipe.fit(X_train, X_train_targetSentiment)

import numpy as np
predictedLogReg = clf_logReg_pipe.predict(X_test)
np.mean(predictedLogReg == X_test_targetSentiment)

#svm
from sklearn.svm import LinearSVC
clf_linearSVC_pipe =  Pipeline([("vect", CountVectorizer()), ("tfidf", TfidfTransformer()), ("clf_linearSVC", LinearSVC())])
clf_linearSVC_pipe.fit(X_train, X_train_targetSentiment)

predictedLinearSVC = clf_linearSVC_pipe.predict(X_test)
np.mean(predictedLinearSVC == X_test_targetSentiment)

#random forest
from sklearn.ensemble import RandomForestClassifier
clf_randomForest_pipe = Pipeline([("vect", CountVectorizer()), ("tfidf", TfidfTransformer()), ("clf_randomForest", RandomForestClassifier())])

clf_randomForest_pipe.fit(X_train, X_train_targetSentiment)
predictedRandomForest = clf_randomForest_pipe.predict(X_test)
np.mean(predictedRandomForest == X_test_targetSentiment)

from sklearn.model_selection import GridSearchCV
parameters = {'vect__ngram_range': [(1, 1), (1, 2)],    
             'tfidf__use_idf': (True, False), 
             } 
gs_clf_LinearSVC_pipe = GridSearchCV(clf_linearSVC_pipe, parameters, n_jobs=-1)
gs_clf_LinearSVC_pipe = gs_clf_LinearSVC_pipe.fit(X_train, X_train_targetSentiment)
new_text = ["The tablet is good, really liked it.", # positive
            "The tablet is ok, but it works fine.", # neutral
            "The tablet is not good, does not work very well."] # negative

X_train_targetSentiment[gs_clf_LinearSVC_pipe.predict(new_text)]